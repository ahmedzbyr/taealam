<!-- BEGIN_TF_DOCS -->

# Datastream Module

In this module we will be creating a datastream.

## Inputs

| Name | Description | Type | Default | Required |
|------|-------------|------|---------|:--------:|
| <a name="input_destination_connection_profile"></a> [destination\_connection\_profile](#input\_destination\_connection\_profile) | Destination connection profile resource. Format: `projects/{project}/locations/{location}/connectionProfiles/{name}` | `any` | n/a | yes |
| <a name="input_display_name"></a> [display\_name](#input\_display\_name) | Display name. | `string` | n/a | yes |
| <a name="input_labels"></a> [labels](#input\_labels) | Labels for the connection. <br><br>Note: This field is non-authoritative, and will only manage the labels present in your configuration. Also please refer to the field `effective_labels` for all of the labels present on the resource.<br><br>`effective_labels` - All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Terraform, other clients and services. | `any` | n/a | yes |
| <a name="input_location"></a> [location](#input\_location) | The name of the location this stream is located in. | `string` | n/a | yes |
| <a name="input_project"></a> [project](#input\_project) | The ID of the project in which the resource belongs. | `string` | n/a | yes |
| <a name="input_source_connection_profile"></a> [source\_connection\_profile](#input\_source\_connection\_profile) | Source connection profile resource. Format: `projects/{project}/locations/{location}/connectionProfiles/{name}` | `any` | n/a | yes |
| <a name="input_stream_id"></a> [stream\_id](#input\_stream\_id) | The stream identifier. | `string` | n/a | yes |
| <a name="input_backfill_all"></a> [backfill\_all](#input\_backfill\_all) | Backfill strategy to automatically backfill the Stream's objects. Specific objects can be excluded.<br><br>  - `mysql_excluded_objects` - MySQL data source objects to avoid backfilling.<br>  - `postgresql_excluded_objects` - PostgreSQL data source objects to avoid backfilling.<br>  - `oracle_excluded_objects` - PostgreSQL data source objects to avoid backfilling. <br><br>**`mysql_excluded_objects`**<pre>{<br>    mysql_excluded_objects = {<br>      mysql_databases = [{<br>        database     = string<br>        mysql_tables = [{<br>          table         = string<br>          mysql_columns = [{<br>            column           = string<br>            data_type        = string<br>            collation        = string<br>            primary_key      = boolean<br>            nullable         = boolean<br>            ordinal_position = integer<br>          }]<br>        }]<br>      }]<br>    }<br>  }</pre>**`postgresql_excluded_objects`**<pre>{<br>    postgresql_excluded_objects = {<br>      postgresql_schemas = [{<br>        schema     = string<br>        postgresql_tables = [{<br>          table           = string<br>          postgresql_columns = [{<br>            column           = string<br>            data_type        = string<br>            primary_key      = boolean<br>            nullable         = boolean<br>            ordinal_position = integer<br>          }]<br>        }]<br>      }]<br>    }<br>  }</pre>**`oracle_excluded_objects`**<pre>{<br>    oracle_excluded_objects = {<br>      oracle_schemas = [{<br>        schema     = string<br>        oracle_tables = [{<br>          table         = string<br>          oracle_columns = [{<br>            column           = string<br>            data_type        = string<br>          }]<br>        }]<br>      }]<br>    }<br>  }</pre> | `any` | `null` | no |
| <a name="input_backfill_none"></a> [backfill\_none](#input\_backfill\_none) | Backfill strategy to disable automatic backfill for the Stream's objects. | `bool` | `false` | no |
| <a name="input_bigquery_destination_config"></a> [bigquery\_destination\_config](#input\_bigquery\_destination\_config) | A configuration for how data should be loaded to Bigquery Dataset. <br><br>- `data_freshness` - The guaranteed data freshness (in seconds) when querying tables created by the stream. Editing this field will only affect new tables created in the future, but existing tables will not be impacted. Lower values mean that queries will return fresher data, but may result in higher cost. A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s". Defaults to 900s.<br>- `single_target_dataset` - A single target dataset to which all data will be streamed. Structure is documented below.<br>- `source_hierarchy_datasets` - Destination datasets are created so that hierarchy of the destination data objects matches the source hierarchy. Structure is documented below.<br><br>**`single_target_dataset`**<br>- `dataset_id` - (Required) Dataset ID in the format `projects/{project}/datasets/{dataset_id}` or `{project}:{dataset_id}`<br> <br>**`source_hierarchy_datasets`**<br>- `dataset_template` - (Required) Dataset template used for dynamic dataset creation.<br><br>**`dataset_template`**<br>- `location` - (Required) The geographic location where the dataset should reside. See [locations](https://cloud.google.com/bigquery/docs/locations) for supported locations.<br>- `dataset_id_prefix` - (Optional) If supplied, every created dataset will have its name prefixed by the provided value. The prefix and name will be separated by an underscore. i.e. `_`.<br>- `kms_key_name` - (Optional) Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table. The BigQuery Service Account associated with your project requires access to this encryption key. i.e. `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{cryptoKey}`.<br><br>**JSON Representation**<pre>{<br>  "data_freshness": string,<br>  <br>  // Union field dataset_config can be only one of the following:<br>  "single_target_dataset": {<br>    dataset_id : string <br>  },<br>  "source_hierarchy_datasets": {<br>    dataset_template : {<br>      location : string <br>      dataset_id_prefix : string <br>      kms_key_name : string<br>    }<br>  }<br>  // End of list of possible types for union field dataset_config.<br>}</pre> | `any` | `null` | no |
| <a name="input_customer_managed_encryption_key"></a> [customer\_managed\_encryption\_key](#input\_customer\_managed\_encryption\_key) | A reference to a KMS encryption key. If provided, it will be used to encrypt the data. If left blank, data will be encrypted using an internal Stream-specific encryption key provisioned through KMS. | `string` | `null` | no |
| <a name="input_desired_state"></a> [desired\_state](#input\_desired\_state) | Desired state of the Stream. Set this field to `RUNNING` to start the stream, and `PAUSED` to pause the stream. | `string` | `"RUNNING"` | no |
| <a name="input_gcs_destination_config"></a> [gcs\_destination\_config](#input\_gcs\_destination\_config) | A configuration for how data should be loaded to Cloud Storage.<br><br>- `path` - Path inside the Cloud Storage bucket to write data to.<br>- `file_rotation_mb`       - The maximum file size to be saved in the bucket.<br>- `file_rotation_interval` - The maximum duration for which new events are added before a file is closed and a new file is created. A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s". Defaults to 900s.<br>- `avro_file_format` - AVRO file format configuration.<br>- `json_file_format` - JSON file format configuration.<br><br>**`json_file_format`**<br>- `schema_file_format` - The schema file format along `JSON` data files. Possible values are: `NO_SCHEMA_FILE`, `AVRO_SCHEMA_FILE`.<br>- `compression`        - Compression of the loaded `JSON` file. Possible values are: `NO_COMPRESSION`, `GZIP`.<br><br>**JSON representation**<pre>{<br>  "path": string,<br>  "file_rotation_mb": integer,<br>  "file_rotation_interval": string,<br>  <br>  // Union field file_format can be only one of the following:<br>  "avro_file_format": {}, // This type has no fields.<br>  "json_file_format": {<br>    {<br>      "schema_file_format": enum <br>      "compression": enum <br>    }<br>  }<br>  // End of list of possible types for union field file_format.<br>}</pre> | `any` | `null` | no |
| <a name="input_mysql_source_config"></a> [mysql\_source\_config](#input\_mysql\_source\_config) | MySQL data source configuration.<br><br>- `include_objects` - MySQL objects to retrieve from the source. Structure is documented below.<br>- `exclude_objects` - MySQL objects to exclude from the stream. Structure is documented below.<br>- `max_concurrent_cdc_tasks` - Maximum number of concurrent CDC tasks. The number should be non negative. If not set (or set to 0), the system's default value will be used.<br>- `max_concurrent_backfill_tasks` - Maximum number of concurrent backfill tasks. The number should be non negative. If not set (or set to 0), the system's default value will be used. <br><br>**`include_objects` and `exclude_objects`**<pre>{<br>  *clude_objects = {<br>    mysql_databases = [{<br>      database     = string<br>      mysql_tables = [{<br>        table         = string<br>        mysql_columns = [{<br>          column           = string<br>          data_type        = string<br>          collation        = string<br>          primary_key      = boolean<br>          nullable         = boolean<br>          ordinal_position = integer<br>        }]<br>      }]<br>    }]<br>  }<br>}</pre> | `any` | `null` | no |
| <a name="input_oracle_source_config"></a> [oracle\_source\_config](#input\_oracle\_source\_config) | Oracle data source configuration.<br><br>- `include_objects` - Oracle objects to retrieve from the source. Structure is documented below.<br>- `exclude_objects` - Oracle objects to exclude from the stream. Structure is documented below.<br>- `max_concurrent_cdc_tasks` - Maximum number of concurrent CDC tasks. The number should be non negative. If not set (or set to 0), the system's default value will be used.<br>- `max_concurrent_backfill_tasks` - Maximum number of concurrent backfill tasks. The number should be non negative. If not set (or set to 0), the system's default value will be used. <br>- `drop_large_objects` - Configuration to drop large object values.<br>- `stream_large_objects` - Configuration to drop large object values.<br><br>**include\_objects and exclude\_objects**<pre>{<br>  *clude_objects = {<br>    oracle_schemas = [{<br>      schema     = string<br>      oracle_tables = [{<br>        table         = string<br>        oracle_columns = [{<br>          column           = string<br>          data_type        = string<br>        }]<br>      }]<br>    }]<br>  }<br>}</pre> | `any` | `null` | no |
| <a name="input_postgresql_source_config"></a> [postgresql\_source\_config](#input\_postgresql\_source\_config) | Oracle data source configuration.<br><br>- `include_objects` - Oracle objects to retrieve from the source. Structure is documented below.<br>- `exclude_objects` - Oracle objects to exclude from the stream. Structure is documented below.<br>- `replication_slot` - (Required) The name of the logical replication slot that's configured with the pgoutput plugin.<br>- `publication` - (Required) The name of the publication that includes the set of all tables that are defined in the stream's include\_objects.<br>- `max_concurrent_backfill_tasks` - Maximum number of concurrent backfill tasks. The number should be non negative. If not set (or set to 0), the system's default value will be used. <br><br>**`include_objects` and `exclude_objects`**<pre>{<br>  *clude_objects = {<br>    postgresql_schemas = [{<br>      schema     = string<br>      postgresql_tables = [{<br>        table           = string<br>        postgresql_columns = [{<br>          column           = string<br>          data_type        = string<br>          primary_key      = boolean<br>          nullable         = boolean<br>          ordinal_position = integer<br>        }]<br>      }]<br>    }]<br>  }<br>}</pre> | `any` | `null` | no |

## Outputs

| Name | Description |
|------|-------------|
| <a name="output_this_stream_effective_labels"></a> [this\_stream\_effective\_labels](#output\_this\_stream\_effective\_labels) | All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Terraform, other clients and services. |
| <a name="output_this_stream_id"></a> [this\_stream\_id](#output\_this\_stream\_id) | An identifier for the resource with format `projects/{{project}}/locations/{{location}}/streams/{{stream_id}}` |
| <a name="output_this_stream_name"></a> [this\_stream\_name](#output\_this\_stream\_name) | The resource's name. |
| <a name="output_this_stream_state"></a> [this\_stream\_state](#output\_this\_stream\_state) | State of the Stream. |
| <a name="output_this_stream_terraform_labels"></a> [this\_stream\_terraform\_labels](#output\_this\_stream\_terraform\_labels) | The combination of labels configured directly on the resource and default labels configured on the provider. |

## Providers

| Name | Version |
|------|---------|
| <a name="provider_google"></a> [google](#provider\_google) | >= 5.2.0 |
| <a name="provider_null"></a> [null](#provider\_null) | n/a |

## Requirements

| Name | Version |
|------|---------|
| <a name="requirement_google"></a> [google](#requirement\_google) | >= 5.2.0 |

## Modules

No modules.

## Resources

| Name | Type |
|------|------|
| [google_datastream_stream.main](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/datastream_stream) | resource |
| [null_resource.check_back_fill](https://registry.terraform.io/providers/hashicorp/null/latest/docs/resources/resource) | resource |

---

---
<!-- END_TF_DOCS -->    